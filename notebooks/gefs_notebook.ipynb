{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "This Jupyter notebook provides a streamlined approach to managing GEFS (Global Ensemble Forecast System) data using Python. It focuses on checking for the existence of GEFS files and automatically downloading them if they are missing. The notebook removes any dependencies on Plotly, ensuring a simpler and more efficient workflow.\n",
    "\n",
    "## Setup and Imports\n",
    "\n",
    "First, install and import the necessary libraries."
   ],
   "id": "80ff085ad47bf797"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from herbie import Herbie"
   ],
   "id": "59c39c5e748889a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GEFSDataManager Class\n",
    "\n",
    "The `GEFSDataManager` class handles the management of GEFS data files. It checks for file existence and downloads missing files using the Herbie API."
   ],
   "id": "2368f79d832b703a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GEFSDataManager:\n",
    "    def __init__(self, save_dir: str, model: str = \"gefs\", product: str = \"atmos.25\", member: str = \"c00\"):\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.model = model\n",
    "        self.product = product\n",
    "        self.member = member\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_file_path(self, inittime: datetime.datetime, fxx: int, q_str: str) -> Path:\n",
    "        date_str = inittime.strftime('%Y%m%d')\n",
    "        time_str = inittime.strftime('%Hz')\n",
    "        \n",
    "        date_dir = self.save_dir / date_str\n",
    "        date_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        subset_id = \"f412b1fe_c00\"\n",
    "        filename = f\"subset_{subset_id}.{time_str}.pgrb2s.{self.product}.f{fxx:03d}.nc\"\n",
    "        file_path = date_dir / filename\n",
    "        \n",
    "        return file_path\n",
    "    \n",
    "    def ensure_file(self, inittime: datetime.datetime, fxx: int, q_str: str) -> Path:\n",
    "        file_path = self.get_file_path(inittime, fxx, q_str)\n",
    "        if not file_path.exists():\n",
    "            self.download_file(inittime, fxx, q_str, file_path)\n",
    "        return file_path\n",
    "    \n",
    "    def download_file(self, inittime: datetime.datetime, fxx: int, q_str: str, file_path: Path, retries: int = 3, delay: int = 5):\n",
    "        attempt = 0\n",
    "        while attempt < retries:\n",
    "            try:\n",
    "                H = Herbie(\n",
    "                    inittime,\n",
    "                    model=self.model,\n",
    "                    product=self.product,\n",
    "                    fxx=fxx,\n",
    "                    member=self.member\n",
    "                )\n",
    "                data_result = H.xarray(q_str, remove_grib=True)\n",
    "                \n",
    "                # Check if result is a list and handle accordingly\n",
    "                if isinstance(data_result, list):\n",
    "                    ds_combined = xr.merge(data_result, compat='override')\n",
    "                else:\n",
    "                    ds_combined = data_result\n",
    "                \n",
    "                ds_combined.to_netcdf(file_path)\n",
    "                return\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                if attempt < retries:\n",
    "                    time.sleep(delay)\n",
    "                else:\n",
    "                    raise\n",
    "    \n",
    "    def load_data(self, inittime: datetime.datetime, fxx: int, q_str: str) -> xr.Dataset:\n",
    "        file_path = self.ensure_file(inittime, fxx, q_str)\n",
    "        try:\n",
    "            ds = xr.open_dataset(file_path)\n",
    "            return ds\n",
    "        except Exception as e:\n",
    "            raise\n",
    "    \n",
    "    def ensure_files(self, inittime: datetime.datetime, fxx_list: List[int], q_str: str):\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            future_to_fxx = {executor.submit(self.ensure_file, inittime, fxx, q_str): fxx for fxx in fxx_list}\n",
    "            for future in as_completed(future_to_fxx):\n",
    "                try:\n",
    "                    future.result()\n",
    "                except Exception as e:\n",
    "                    pass"
   ],
   "id": "c982eb13af5956cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GEFSData Class\n",
    "\n",
    "The `GEFSData` class utilizes the `GEFSDataManager` to generate timeseries data from GEFS datasets."
   ],
   "id": "fee0e4131b8bcb7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GEFSData:\n",
    "    def __init__(self, data_manager: GEFSDataManager):\n",
    "        self.data_manager = data_manager\n",
    "    \n",
    "    @classmethod\n",
    "    def generate_timeseries(cls, data_manager: GEFSDataManager, fxx: List[int], inittime: datetime.datetime,\n",
    "                            gefs_regex: str, ds_key: str, lat: float, lon: float,\n",
    "                            product: str, member: str = \"c00\", remove_grib: bool = True) -> pd.DataFrame:\n",
    "        \n",
    "        timeseries = []\n",
    "        validtimes = []\n",
    "        \n",
    "        for f in fxx:\n",
    "            data_manager.ensure_file(inittime, f, gefs_regex)\n",
    "            \n",
    "            validtime = inittime + datetime.timedelta(hours=f)\n",
    "            H = cls.setup_herbie(inittime, fxx=f, product=product, model=\"gefs\", member=member)\n",
    "            ds = cls.get_CONUS(gefs_regex, H, remove_grib=remove_grib)\n",
    "            ds_crop = cls.crop_to_UB(ds)\n",
    "            val = cls.get_closest_point(ds_crop, ds_key, lat, lon)\n",
    "            validtimes.append(validtime)\n",
    "            timeseries.append(val.values)\n",
    "        \n",
    "        ts_df = pd.DataFrame({ds_key: timeseries}, index=validtimes)\n",
    "        return ts_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_herbie(inittime: datetime.datetime, fxx: int = 0, product: str = \"atmos.25\", model: str = \"gefs\", member: str = 'c00') -> Herbie:\n",
    "        H = Herbie(\n",
    "            inittime,\n",
    "            model=model,\n",
    "            product=product,\n",
    "            fxx=fxx,\n",
    "            member=member\n",
    "        )\n",
    "        return H\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_CONUS(qstr: str, herbie_inst: Herbie, remove_grib: bool = True) -> xr.Dataset:\n",
    "        result = herbie_inst.xarray(qstr, remove_grib=remove_grib)\n",
    "        \n",
    "        # Check if result is a list and merge if necessary\n",
    "        if isinstance(result, list):\n",
    "            ds_combined = xr.merge(result, compat='override')\n",
    "        else:\n",
    "            ds_combined = result\n",
    "        \n",
    "        # Parse with MetPy\n",
    "        ds_combined = ds_combined.metpy.parse_cf()\n",
    "        \n",
    "        return ds_combined\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_closest_point(ds: xr.Dataset, vrbl: str, lat: float, lon: float) -> xr.DataArray:\n",
    "        point_val = ds[vrbl].sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "        return point_val\n",
    "    \n",
    "    @staticmethod\n",
    "    def crop_to_UB(ds: xr.Dataset) -> xr.Dataset:\n",
    "        sw_corner = (39.4, -110.9)\n",
    "        ne_corner = (41.1, -108.5)\n",
    "        \n",
    "        lats = ds.latitude.values\n",
    "        lons = ds.longitude.values\n",
    "        \n",
    "        if np.max(lons) > 180.0:\n",
    "            lons -= 360.0\n",
    "        \n",
    "        ds_sub = ds.sel(latitude=slice(ne_corner[0], sw_corner[0]),\n",
    "                       longitude=slice(sw_corner[1], ne_corner[1]))\n",
    "        \n",
    "        return ds_sub"
   ],
   "id": "1a04e9468ee85cfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example Usage\n",
    "\n",
    "Here's how to utilize the `GEFSDataManager` and `GEFSData` classes to check for GEFS files and download them if necessary. We'll also generate a timeseries dataset from the downloaded files."
   ],
   "id": "48bff0b926e59705"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the data manager.\n",
    "save_directory_example= \"notebooks/gefs_data\"\n",
    "data_manager_example= GEFSDataManager(save_dir=save_directory_example)\n",
    "\n",
    "# Define initialization time and forecast hours.\n",
    "init_time_example= datetime.datetime(2024 ,11 ,6 ,18 ) # Example date and time.\n",
    "forecast_hours_example= [0 ,6 ,12 ,18 ]\n",
    "\n",
    "# Define query string for Herbie.\n",
    "query_string_example= \"t\"\n",
    "\n",
    "# Ensure all required files are available.\n",
    "data_manager_example.ensure_files(inittime=init_time_example,fxx_list=forecast_hours_example,q_str=query_string_example)\n",
    "\n",
    "# Initialize GEFSData instance and generate timeseries data for a specific location.\n",
    "gefs_data_instance_example= GEFSData(data_manager=data_manager_example)\n",
    "\n",
    "latitude_example= 40.0 # Example latitude value.\n",
    "longitude_example= -109.0 # Example longitude value.\n",
    "dataset_key_example=\"t\"\n",
    "\n",
    "timeseries_data_frame_example= gefs_data_instance_example.generate_timeseries(\n",
    "    data_manager=data_manager_example,\n",
    "    fxx=forecast_hours_example,\n",
    "    inittime=init_time_example,\n",
    "    gefs_regex=query_string_example,\n",
    "    ds_key=dataset_key_example,\n",
    "    lat=latitude_example,\n",
    "    lon=longitude_example,\n",
    "    product=\"atmos.25\"\n",
    ")\n",
    "\n",
    "# Display the timeseries DataFrame.\n",
    "print(timeseries_data_frame_example)"
   ],
   "id": "de9f3e60c0363203",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
