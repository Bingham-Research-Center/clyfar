# Possibilistic verification ideas for Clyfar

Goal: evaluate Clyfar operationally using its **possibility outputs** (not crisp yes/no; not probabilities), with emphasis on â€œreduced exceedance surpriseâ€ and honest communication of **ignorance** (subnormality / â€œunsureâ€ mass).

## Notation (per forecast instance `t`)

- Observation (daily max ozone): `y_t âˆˆ â„` (ppb) or ordered category `Ï‰_t âˆˆ Î©`.
- Forecast possibility distribution on ozone values: `Ï€_t(z) âˆˆ [0,1]`, possibly **subnormal**.
  - `m_t = sup_z Ï€_t(z) â‰¤ 1`, `I_t = 1 - m_t` (ignorance / â€œunsureâ€).
- `Î±`-cut set: `S_t(Î±) = { z : Ï€_t(z) â‰¥ Î± }` for `Î± âˆˆ [0, m_t]`.
  - For unimodal outputs, `S_t(Î±)` is typically an interval `[â„“_t(Î±), u_t(Î±)]`.
- Exceedance event for threshold `T`: `A_T = { z â‰¥ T }`
  - Upper plausibility: `Î _t(T) = Î _t(A_T) = sup_{zâ‰¥T} Ï€_t(z)`
  - Lower certainty: `N_t(T) = N_t(A_T) = 1 - sup_{z<T} Ï€_t(z)`

---

## 1) A score for possibilistic forecasts (distribution-level, not defuzzified)

### Proposal 1A: `Î±`-cut weighted interval score (set-based; sharpness + miss penalty)

Treat a possibility distribution as a **nested family of prediction sets**. Score those sets.

Choose a grid `ğ’œ = {Î±_1, â€¦, Î±_K}` (e.g., `Î±_k = 0.05k`).

For each `Î± âˆˆ ğ’œ`, compute the `Î±`-cut interval `S_t(Î±) = [â„“_t(Î±), u_t(Î±)]` and define the (Gneitingâ€“Raftery) interval score:

`IS_t(Î±) = (u_t(Î±)-â„“_t(Î±)) + (2/Î±)Â·(â„“_t(Î±)-y_t)_+ + (2/Î±)Â·(y_t-u_t(Î±))_+`

Then define the possibilistic score (lower is better):

`PWIS_t = ÎºÂ·I_t + Î£_{Î±âˆˆğ’œ} w(Î±)Â·IS_t(Î±)`

Recommended weights: `w(Î±) âˆ Î±` (punish misses in the high-possibility â€œcoreâ€ more than in the low-possibility â€œtailsâ€).

Why itâ€™s possibility-native (and â€œinformation-ishâ€ without Shannon/KL):
- The **sharpness term** integrates set sizes: `âˆ« |S_t(Î±)| dÎ± = âˆ« Ï€_t(z) dz` (Fubini). That is a maxitive/nested-set analogue of â€œnonspecificityâ€: small area under `Ï€` = more specific forecast.
- The **miss penalties** ask: how far outside the plausible set did reality fall, and at what plausibility level?
- `I_t` explicitly prices subnormal â€œI donâ€™t knowâ€.

Categorical / binned variant (ordered `Î©` or bins):
- `S_t(Î±) = { Ï‰ âˆˆ Î© : Ï€_t(Ï‰) â‰¥ Î± }`
- Replace width with set size and miss indicator:
  - `IS^cat_t(Î±) = |S_t(Î±)|/|Î©| + (1/Î±)Â·1{Ï‰_t âˆ‰ S_t(Î±)}`
- `PWIS^cat_t = ÎºÂ·I_t + Î£ w(Î±)Â·IS^cat_t(Î±)`

### Proposal 1B (simpler): contradiction + nonspecificity + ignorance

If you want something you can compute even without extracting `Î±`-cuts cleanly:
- â€œContradictionâ€ of the realized value: `C_t = 1 - Ï€_t(y_t)` (or `-log(Ï€_t(y_t)+Îµ)`)
- â€œNonspecificityâ€ (area under the curve): `NS_t = (1/L)âˆ« Ï€_t(z) dz` where `L` is domain length (or `NS_t = (1/|Î©|)Î£_Ï‰ Ï€_t(Ï‰)` for categories)

`Score_t = C_t + Î»Â·NS_t + ÎºÂ·I_t`

This is a compact â€œbe right, be sharp, admit ignoranceâ€ loss.

---

## 2) â€œClimatologyâ€ in possibility space (for skill scores)

You want a baseline `Ï€_clim` that is **not** a probability forecast, but still yields nested sets to compare against.

### Proposal 2A: climatology as quantile-induced consonant possibility

Let `F_clim(z)` be the climatological CDF for the relevant season/regime (e.g., winter only; optionally day-of-year smoothed).

Define a climatological possibility distribution:

`Ï€_clim(z) = 1 - 2Â·|F_clim(z) - 0.5|`

Then the `Î±`-cuts are exactly climatological central quantile intervals:

`S_clim(Î±) = { z : Ï€_clim(z) â‰¥ Î± } = [Q_clim(Î±/2), Q_clim(1-Î±/2)]`

This is operationally handy because it makes â€œclimatologyâ€ look like a nested plausibility family (same object type as Clyfarâ€™s `Î±`-cuts), without interpreting `Î±` as probability.

### Proposal 2B: frequency-normalized categorical climatology

For ordered categories `Î©` (or ppb bins), let `f(Ï‰)` be the empirical frequency in the archive.

`Ï€_clim(Ï‰) = f(Ï‰) / max_{Ï‰'} f(Ï‰')`

(Optionally smooth across adjacent bins/categories if you want unimodality.)

### Skill score

For any loss where â€œlower is betterâ€ (e.g., `PWIS`):

`Skill = 1 - mean_t(Score_model) / mean_t(Score_clim)`

Compute per lead `â„“` as well: `Skill(lead=â„“)` to show horizon decay (the operational question).

---

## 3) Like-for-like comparisons with probabilistic and quantile forecasts (without KL/Brier)

Key trick: compare everything as **nested prediction sets** `S_t(Î±)`, then score those sets (e.g., `PWIS`).

### 3A: put probability forecasts into the same â€œnested-setâ€ shape

Given a probabilistic forecast CDF `F_t`, define:

`S^prob_t(Î±) = [Q_t(Î±/2), Q_t(1-Î±/2)]`

Now `S^prob_t(Î±)` and Clyfarâ€™s `S_t(Î±)` are both just families of intervals indexed by `Î±`, so you can apply **the same** `PWIS` definition to both.

If you only have quantiles (e.g., 10/50/90):
- Score only the corresponding `Î±` levels you have (e.g., 10â€“90 is central 80% â‡’ `Î±=0.2`) using the interval-score piece at those `Î±`.
- This is a â€œpartial PWISâ€ analogous to a reduced WIS; still apples-to-apples if you apply the same `Î±` set to all methods.

### 3B: a common â€œdepth-of-truthâ€ index

Define the maximum plausibility level whose cut-set still contains the observation:

`Î±*_t = sup{ Î± : y_t âˆˆ S_t(Î±) }`

For Clyfar, `Î±*_t = Ï€_t(y_t)`.
For probabilistic `F_t` using central intervals, `Î±*_t = 1 - 2Â·|F_t(y_t) - 0.5|`.

This gives a like-for-like, unitless â€œhow deep into the core did reality land?â€ quantity that can be summarized (mean, quantiles) vs lead time, without ever treating forecasts as additive probabilities.

### 3C: exceedance-focused comparison (surprise reduction for operational thresholds)

For a threshold `T`, define a binary outcome `e_t = 1{y_t â‰¥ T}` and score **plausibility + certainty + informativeness**:

`L_t(T) = e_tÂ·[-log(Î _t(T)+Îµ)] + (1-e_t)Â·[-log(1-N_t(T)+Îµ)] + Î»Â·(Î _t(T)-N_t(T)) + ÎºÂ·I_t`

Compute `mean_t L_t(T)` for operational thresholds (`T = 60, 70, 75 ppb`, etc.) and per lead time. This directly answers: â€œdid the model stop being *surprised* by exceedances earlier than climatology?â€

Optional (Clyfar-specific) â€œself-awarenessâ€ check:
- Does `I_t` predict when Clyfar will be wrong?
  - Example metric: Spearman `Ï( I_t, |Ä‰_t - y_t| )`, where `Ä‰_t` is the centroid (or any scalar summary you already compute).
  - Or treat â€œlarge errorâ€ as an event and compute AUC using `I_t` as the ranking score (abstention/quality-control value).

---

## Clarifications that affect which score is best

1. Do you want verification on the continuous `Ï€_t(z)` curve, or only on 4 categories (`background/moderate/elevated/extreme`)?
2. Should subnormality be rewarded as â€œhonest abstentionâ€ (small `Îº`), or penalized as â€œnot operationally usefulâ€ (large `Îº`)?
3. Which exceedance thresholds matter operationally (e.g., `T=70 ppb` only, or multiple)?
