# Possibilistic verification ideas for Clyfar (plain language + math)

Goal: evaluate Clyfar operationally when it outputs **possibilities** (what is plausible) rather than **probabilities** (what is likely). We want to reward forecasts that (1) put the observation inside their â€œplausible rangeâ€, (2) stay as tight as possible when confident, and (3) honestly say â€œIâ€™m unsureâ€ when the rules donâ€™t support any outcome (subnormality).

## Notation (one forecast case, indexed by `t`)

- Observation: daily max ozone `y_t` (ppb). If you verify in ordered bins/categories, write the observed category as `Ï‰_t âˆˆ Î©`.
- Possibility forecast on ozone values: `Ï€_t(z) âˆˆ [0,1]`.
  - Read `Ï€_t(z)` as â€œhow compatible is value `z` with the inputs + fuzzy rulesâ€. It is **not** a probability, so it does not need to add up to 1.
  - Subnormality: `m_t = sup_z Ï€_t(z) â‰¤ 1`. Ignorance (an â€œunsureâ€ mass): `I_t = 1 - m_t`.
- `Î±`-cut (a â€œplausible setâ€ at strictness level `Î±`): `S_t(Î±) = { z : Ï€_t(z) â‰¥ Î± }` for `Î± âˆˆ [0, m_t]`.
  - Bigger `Î±` means â€œI only keep values the model finds *very* plausibleâ€, so `S_t(Î±)` shrinks as `Î±` increases (nested sets).
  - If `Ï€_t` is roughly single-peaked, `S_t(Î±)` is usually an interval `[â„“_t(Î±), u_t(Î±)]`.
- Exceedance above a threshold `T`: `A_T = { z â‰¥ T }`.
  - Possibility of exceedance (upper plausibility): `Î _t(T) = sup_{zâ‰¥T} Ï€_t(z)`.
  - Necessity of exceedance (lower certainty): `N_t(T) = 1 - sup_{z<T} Ï€_t(z)`.
  - Plain language: `Î _t(T)` answers â€œcould `y_t â‰¥ T` happen?â€ and `N_t(T)` answers â€œis `y_t â‰¥ T` basically unavoidable?â€.

---

## 1) A score for possibilistic forecasts (use the whole curve, not a single defuzzified number)

### Proposal 1A: `Î±`-cut weighted interval score (PWIS)

Core idea: treat one possibility curve `Ï€_t` as **many prediction intervals** (`Î±`-cuts). Then score those intervals the same way we score quantile intervals in classical forecast verification.

Pick a small set of `Î±` levels, `ğ’œ = {Î±_1, â€¦, Î±_K}` (example: `Î±_k = 0.05k`). For each `Î±`, compute the `Î±`-cut interval `S_t(Î±) = [â„“_t(Î±), u_t(Î±)]`.

For each `Î±`, use the interval score (Gneitingâ€“Raftery):

`IS_t(Î±) = (u_t(Î±)-â„“_t(Î±)) + (2/Î±)Â·(â„“_t(Î±)-y_t)_+ + (2/Î±)Â·(y_t-u_t(Î±))_+`

How to read the three terms:
- `(u-â„“)` rewards **sharpness** (narrower plausible ranges are better).
- The `(_+ )` terms penalize when the observation falls **below** the interval or **above** the interval; the factor `(2/Î±)` makes misses at high `Î±` (the â€œcoreâ€) hurt more.

Combine across `Î±` levels and add an ignorance penalty:

`PWIS_t = ÎºÂ·I_t + Î£_{Î±âˆˆğ’œ} w(Î±)Â·IS_t(Î±)`  (lower is better)

Recommended weights: `w(Î±) âˆ Î±`. Plain language: missing the observation outside the â€œmost plausibleâ€ core should count more than missing it outside the low-plausibility fringe.

Why this is a natural â€œinformationâ€ proxy for possibility (without Shannon/KL):
- Summing widths across `Î±` levels measures how *spread out* the curve is. In fact, â€œtotal width across all `Î±`â€ is mathematically equivalent to â€œarea under `Ï€_t(z)`â€. Smaller area â‡’ the forecast rules out more values â‡’ more specific guidance.
- The miss penalties measure how badly reality contradicts what the model called plausible at each strictness level.
- `I_t` explicitly charges the model when it says â€œI donâ€™t knowâ€ (you set `Îº` based on how much you want to discourage abstention).

Quick computation recipe (continuous `z`):
1. Choose `ğ’œ` and weights `w(Î±)`.
2. For each `Î±`, compute `[â„“_t(Î±), u_t(Î±)]` from the curve.
3. Compute `IS_t(Î±)` and sum with weights.
4. Add `ÎºÂ·I_t`.

Categorical/binned version (if you only have `Î©` categories or ppb bins):
- `S_t(Î±) = { Ï‰ âˆˆ Î© : Ï€_t(Ï‰) â‰¥ Î± }`
- Replace â€œinterval widthâ€ with â€œset sizeâ€, and use a miss indicator:
  - `IS^cat_t(Î±) = |S_t(Î±)|/|Î©| + (1/Î±)Â·1{Ï‰_t âˆ‰ S_t(Î±)}`
- `PWIS^cat_t = ÎºÂ·I_t + Î£_{Î±âˆˆğ’œ} w(Î±)Â·IS^cat_t(Î±)`

### Proposal 1B (lighter-weight): contradiction + spread + ignorance

If extracting `Î±`-cut intervals is annoying, score three simple things:
- Contradiction at the realized value: `C_t = 1 - Ï€_t(y_t)` (or `C_t = -log(Ï€_t(y_t)+Îµ)` if you want â€œsurprise-likeâ€ growth near 0).
- Spread / nonspecificity: `NS_t = (1/L)âˆ« Ï€_t(z) dz` where `L` is the `z`-domain length (or `NS_t = (1/|Î©|)Î£_Ï‰ Ï€_t(Ï‰)` for categories).
- Ignorance: `I_t = 1 - sup_z Ï€_t(z)`.

Combine:

`Score_t = C_t + Î»Â·NS_t + ÎºÂ·I_t`  (lower is better)

Plain language: â€œbe rightâ€ (`C_t` small), â€œbe sharpâ€ (`NS_t` small), and â€œdonâ€™t hide behind uncertaintyâ€ (`I_t` small unless you truly need it).

---

## 2) â€œClimatologyâ€ in possibility space (baseline for skill)

We need a baseline forecast of the *same type* (a possibility curve or nested plausible sets), so the skill score compares apples-to-apples.

### Proposal 2A: build a climatology possibility curve from the climatological CDF

Let `F_clim(z)` be the climatological CDF for the relevant regime (e.g., winter only; optionally smooth by day-of-year).

Define:

`Ï€_clim(z) = 1 - 2Â·|F_clim(z) - 0.5|`

Plain language: values near the climatological median are â€œmost plausibleâ€ (`Ï€â‰ˆ1`), and values deep in either tail are â€œleast plausibleâ€ (`Ï€â‰ˆ0`). This creates a simple, symmetric â€œclimatology possibility curveâ€.

Key property (why this is useful): its `Î±`-cuts are exactly central climatological quantile intervals:

`S_clim(Î±) = { z : Ï€_clim(z) â‰¥ Î± } = [Q_clim(Î±/2), Q_clim(1-Î±/2)]`

So your baseline automatically produces nested intervals you can score with the exact same PWIS machinery.

### Proposal 2B: categorical climatology (bins/categories)

If you verify on ordered categories/bins, let `f(Ï‰)` be the empirical frequency of category `Ï‰` in the archive. Define:

`Ï€_clim(Ï‰) = f(Ï‰) / max_{Ï‰'} f(Ï‰')`

Plain language: the most common category gets possibility 1, rarer categories get smaller possibility (optionally smooth across adjacent bins if you want a single-peaked baseline).

### Skill score (relative improvement over climatology)

For any score where â€œlower is betterâ€ (e.g., `PWIS`):

`Skill = 1 - mean_t(Score_model) / mean_t(Score_clim)`

Interpretation: `Skill > 0` means Clyfar beats climatology; `Skill = 0` ties; `Skill < 0` is worse than climatology. Compute by lead time too: `Skill(lead=â„“)`.

---

## 3) Like-for-like comparison with probability and quantile forecasts (without KL/Brier)

Main trick: reduce every method to the same object: a family of nested prediction sets `S_t(Î±)`. Then score those sets.

### 3A: convert a probabilistic forecast into nested central intervals

Given a probabilistic forecast CDF `F_t`, define the central interval:

`S^prob_t(Î±) = [Q_t(Î±/2), Q_t(1-Î±/2)]`

Now a probabilistic model and Clyfar both yield a nested family of intervals indexed by the same `Î±`, so you can apply the same PWIS definition to both.

If you only have a few quantiles (e.g., 10/50/90):
- The 10â€“90 interval is a central 80% interval, which corresponds to `Î±=0.2` in the formula above.
- Score only the `Î±` values you actually have for all models (a â€œpartial PWISâ€), so the comparison stays fair.

### 3B: â€œdepth-of-truthâ€ (one-number summary that works for both)

Define:

`Î±*_t = sup{ Î± : y_t âˆˆ S_t(Î±) }`

Plain language: `Î±*_t` is the strictest level at which the observation is still inside the modelâ€™s â€œplausible setâ€. Bigger means the observation landed deeper in the modelâ€™s core expectations.

For Clyfar: `Î±*_t = Ï€_t(y_t)`.

For a probabilistic `F_t` using central intervals:

`Î±*_t = 1 - 2Â·|F_t(y_t) - 0.5|`

Interpretation: `Î±*_tâ‰ˆ1` if `y_t` is near the predictive median; `Î±*_tâ‰ˆ0` if `y_t` is in an extreme predictive tail. Summarize `Î±*_t` by lead time to show how â€œsurpriseâ€ grows with horizon.

### 3C: exceedance-focused score (reduce â€œexceedance surpriseâ€ for operational thresholds)

For an operational threshold `T`, define the observed event `e_t = 1{y_t â‰¥ T}`. Use possibility/necessity as your forecasted upper/lower bounds for that event:
- `Î _t(T)` = â€œcould exceed `T`?â€
- `N_t(T)` = â€œmust exceed `T`?â€

Score exceedance surprise + uncertainty + ignorance:

`L_t(T) = e_tÂ·[-log(Î _t(T)+Îµ)] + (1-e_t)Â·[-log(1-N_t(T)+Îµ)] + Î»Â·(Î _t(T)-N_t(T)) + ÎºÂ·I_t`

How to read this:
- If an exceedance happens (`e_t=1`) and the model said it was barely possible (`Î _t(T)` small), you pay a big penalty.
- If no exceedance happens (`e_t=0`) and the model said exceedance was nearly certain (`N_t(T)` large), you pay a big penalty.
- `(Î -N)` is the modelâ€™s â€œIâ€™m not sureâ€ interval width for the event; penalizing it encourages more informative forecasts.
- `I_t` still tracks global ignorance/subnormality.

Compute `mean_t L_t(T)` for thresholds like `T = 60, 70, 75 ppb`, and by lead time.

Optional (useful operationally): does Clyfar know when it might be wrong?
- Check whether `I_t` is larger on high-error days, e.g. Spearman `Ï( I_t, |Ä‰_t - y_t| )` where `Ä‰_t` is the centroid/median you already compute.
- Or treat â€œlarge errorâ€ as an event and compute AUC using `I_t` as the ranking score (abstention/quality-control value).

---

## Clarifications (these choices change the â€œrightâ€ score)

1. Verify on continuous `Ï€_t(z)` (ppb curve), or only on a few ozone categories?
2. Should â€œunsureâ€ (`I_t`) be treated as honest abstention (small `Îº`) or as reduced usefulness (large `Îº`)?
3. Which thresholds `T` matter operationally (one threshold like 70 ppb vs a small set)?
