# Review + critique: possibilistic scoring ideas for Clyfar

*Date written: 2026-01-22 (updated after reading scholarium summaries)*  
*Primary target doc: `verif/POSS-SCORE-IDEAS-PLAIN.md`*  
*Key context docs:*  
- `../brc-knowledge/scholarium/active-projects/clyfar/summary-clyfar-prototype.md`  
- `../brc-knowledge/scholarium/active-projects/clyfar/summary-fuzzy-inference.md`  
- `../brc-knowledge/scholarium/active-projects/clyfar/summary-possibility-theory.md`  
- `../brc-knowledge/scholarium/active-projects/clyfar/summary-poss-subnormal.md`  
- (Older background) `../brc-knowledge/archive/2025-11-pre-overhaul/POSSIBILITY-SUBNORMAL-MATH.md`

## 0) What problem are we actually solving?

Clyfar outputs **possibilities** (how compatible outcomes are with rules/evidence), not **probabilities** (how likely outcomes are).

So the verification problem is:

> Given observations `y_t` and possibilistic forecasts `Ï€_t(Â·)`, define a score that rewards forecasts that  
> (i) make the observation â€œplausibleâ€,  
> (ii) are as *specific/sharp* as possible, and  
> (iii) handle **subnormality** (explicit â€œwe donâ€™t knowâ€) in a sensible, non-gameable way.

A stretch goal is an â€œapples-to-applesâ€ comparison against probabilistic forecasts (CDFs/quantiles/ensembles) and/or deterministic forecasts.

## 0.1) Extra context that affects what â€œgood verificationâ€ means

From the four `summary-*` docs, the **purpose** of the possibilistic product is not â€œprobability replacementâ€:

- The guiding question is *conservative and risk-averse*: â€œ**could** a hazardous event happen?â€ rather than â€œhow likely?â€ (`summary-possibility-theory.md`).
- Clyfar is explicitly meant to communicate **second-order uncertainty** (uncertainty about the modelâ€™s own knowledge) via **subnormality** and an â€œunsureâ€ amount (`summary-poss-subnormal.md`).
- In cusp / tipping-point situations, the FIS can output **subnormal** distributions, and the docs treat that as a *feature* (â€œrules donâ€™t strongly support any outcome; be honestâ€) rather than a bug (`summary-clyfar-prototype.md`, `summary-fuzzy-inference.md`).
- A core motivation is that a scalar â€œcentroidâ€ can **mute tail risk**; the possibility distribution may still flag an extreme as unusually plausible even when the centroid forecast is wrong (`summary-clyfar-prototype.md`).

Implication: it may be better to report a **scorecard** (e.g., tail-risk ranking skill + sharpness + ignorance) than to force everything into one â€œprobability-likeâ€ score.

## 1) Minimal definitions that stay consistent under subnormality

### 1.1 Raw vs normalized possibility

Let Clyfar output a *raw* possibility distribution `Ï€_raw(z) âˆˆ [0,1]`.

Define the **subnormality level**

`m = sup_z Ï€_raw(z)`  (so `m âˆˆ [0,1]`)

and the **ignorance / unsure amount**

`I = 1 - m`.

(In the brc-knowledge summaries this same idea also appears as â€œunsureâ€ or `H_Î `; and in the Î /N/U framework itâ€™s the `U` term. Theyâ€™re all aiming at the same concept: â€œhow much the system canâ€™t support any outcome.â€)

If `m > 0`, define the **normalized shape**

`Ï€_norm(z) = Ï€_raw(z) / m`, so `sup_z Ï€_norm(z) = 1`.

This split is important:

- `Ï€_norm` describes *shape / relative preferences* among outcomes.
- `I` describes *how much the system is abstaining / missing evidence*.

### 1.2 Events, possibility, and necessity (the key â€œgotchaâ€)

For any event `A` (example: `A_T = {z â‰¥ T}`),

`Î _norm(A) = sup_{zâˆˆA} Ï€_norm(z)`

and the **classical** necessity is

`N_norm(A) = 1 - Î _norm(A^c)`.

This classical duality **only behaves as intended when the distribution is normalized**.

If you apply `N(A)=1-sup_{zâˆˆA^c} Ï€_raw(z)` directly to a *subnormal* `Ï€_raw`, you can get nonsense like â€œA is almost necessaryâ€ even when â€œA is barely possibleâ€.

Concrete example (first-year-undergrad level):

- Suppose `Ï€_raw(z) â‰¤ 0.3` for *all* `z`. Then `m=0.3`, so `I=0.7`.
- For any non-trivial event `A`, youâ€™ll have `Î _raw(A) â‰¤ 0.3` and `Î _raw(A^c) â‰¤ 0.3`.
- If you define `N_raw(A) = 1 - Î _raw(A^c)`, then `N_raw(A) â‰¥ 0.7`.
- That violates the basic consistency intuition `N(A) â‰¤ Î (A)` (â€œsomething canâ€™t be more certain than it is possibleâ€).

So: **use normalized `Ï€_norm` for necessity-style quantities, and keep ignorance `I` separate**.

Practical alignment with code:

- `postprocesing/possibility_funcs.py` normalizes (maxâ†’1) before computing necessity-like outputs.
- It computes `unsure = 1 - max(raw)` from subnormality.

That same split should appear in verification formulas.

## 2) Critique of the current proposals in `POSS-SCORE-IDEAS-PLAIN.md`

This section is intentionally picky: the goal is to prevent subtle math errors from becoming â€œverification folkloreâ€.

### Issue A â€” Necessity under subnormality (major)

The doc defines for thresholds:

`N_t(T) = 1 - sup_{z<T} Ï€_t(z)`.

If `Ï€_t` is subnormal, this can make `N_t(T)` artificially large even when the model is basically saying â€œI have no ideaâ€.

**Correction**: compute conditional/normalized necessity from `Ï€_norm`, and separately track ignorance `I`.

This matches the â€œtripartiteâ€ communication idea in `summary-poss-subnormal.md`: report (possibility, ignorance, conditional necessity) rather than letting subnormality accidentally turn into â€œfalse certaintyâ€.

### Issue B â€” `Î±` plays two different roles

The â€œinterval scoreâ€ formula used is from probabilistic verification, where `Î±` means a **miscoverage probability** (a target like 10% miss rate for a 90% interval).

In the doc, `Î±` is also used as an **Î±-cut level** in a possibility distribution (â€œkeep points with possibility â‰¥ Î±â€).

Those are *not* the same concept in general.

Why it matters:

- In probabilistic scoring, the coefficient `(2/Î±)` is not arbitrary: itâ€™s what makes the score â€œproperâ€ (incentive-compatible) for interval forecasts at nominal level `1-Î±`.
- In possibility scoring, unless you have a calibrated mapping from Î±-cut levels â†’ empirical coverages, `(2/Î±)` is just a weight choice, not a principled constant.

**Two clean fixes**:

1) **Rename** the possibility cut level to something like `r âˆˆ (0,1]` (â€œrelative plausibility levelâ€), to avoid accidental probability interpretation.
2) If you *want* probabilistic comparability, **calibrate** `r` levels to empirical coverages and then use interval scores at those coverages (details in Â§3B).

### Issue C â€” â€œArea under Ï€â€ as sharpness can be gamed by subnormality

The doc suggests using

`NS = (1/L) âˆ« Ï€(z) dz`

as â€œnonspecificity / spreadâ€.

But if you use the *raw* `Ï€_raw`, making everything smaller (more subnormal) shrinks the area and looks â€œsharperâ€, even though itâ€™s less informative.

**Fix**: compute nonspecificity on the normalized shape, e.g.

`NS_shape = (1/L) âˆ« Ï€_norm(z) dz`

and keep `I` separate.

Equivalent â€œlayer-cakeâ€ view:

`âˆ« Ï€_norm(z) dz = âˆ«_0^1 |S_norm(r)| dr`

where `S_norm(r) = {z : Ï€_norm(z) â‰¥ r}`.

### Issue D â€” Handling `Î± = 0` and `Î± > m`

Any formula with `1/Î±` or `2/Î±` must **exclude** `Î±=0`.

Also, for subnormal cases (`m < 1`), any Î±-cut at `Î± > m` is empty. The doc doesnâ€™t say what to do then.

**Fix**: either

- work with normalized `Ï€_norm` and `r âˆˆ (0,1]`, or
- define Î±-grid as `Î±_k = r_k Â· m_t` (relative cuts), or
- explicitly â€œskip Î±-cuts above `m_t`â€.

### Issue E â€” Discrete categories: â€œwidthâ€ should respect ordering/bin widths

The categorical score suggestion uses `|S(Î±)|/|Î©|` as a width proxy.

This is fine if bins are equal-width and the Î±-cuts always form a contiguous block of ordered categories.

But:

- if bins arenâ€™t equal, â€œsizeâ€ is not â€œwidth in ppbâ€.
- if outputs are multi-modal, `S(Î±)` can be non-contiguous (e.g., `{background, extreme}`), and `|S|` hides that weirdness.

**Fix**: for ordinal categories, consider width as

`width(S) = (max_index(S) - min_index(S)) / (K-1)`

or (better) use actual ppb bin widths if available.

### Issue F â€” Verifying only a defuzzified number is misaligned with the project goal

The broader project context (especially `summary-clyfar-prototype.md`) is that the possibility outputs exist because centroid/defuzzified values can hide:

- tail risk (e.g., â€œextreme category ranked in top few %â€)
- epistemic gaps (subnormality in cusp situations)

So, if the verification target is â€œdoes Clyfar communicate second-order uncertainty well?â€, any metric that collapses to a single ppb value is at best incomplete.

## 3) Scoring methods that (a) respect subnormality, and (b) can compare across forecast types

Iâ€™m proposing several options because your â€œbestâ€ choice depends on what you want the score to mean.

### 3A) Option 0: A tripartite verification scorecard (aligns with the communication goal)

This follows `summary-poss-subnormal.md` directly. For each case `t`, compute:

1) **Ignorance:** `I_t = 1 - sup_z Ï€_raw,t(z)` (how â€œunsureâ€ Clyfar is)
2) **Conditional/normalized shape:** `Ï€_norm,t` (relative plausibilities)
3) **Conditional necessity for key events:** `N_t(A)` computed from `Ï€_norm,t`

Then verify *three separate questions*:

- **Tail-risk ranking:** do high `Î _norm(A_T)` (or high `Ï€_norm(extreme)`) days line up with actual exceedance days?
- **Sharpness:** are the cut-sets of `Ï€_norm` tight when Clyfar is confident?
- **Self-awareness:** is `I_t` larger on hard/low-skill days?

This is often easier to interpret (and harder to game) than a single combined score.

### 3A) Option 1: Normalized cut-set score + explicit ignorance penalty (simple, robust)

This is the â€œfix the math, keep the spiritâ€ version of PWIS.

1) Split `Ï€_raw` into `Ï€_norm` and `I`.
2) Choose a grid of **relative cut levels** `r âˆˆ ğ“¡ âŠ‚ (0,1]` (example: `ğ“¡ = {0.1,0.2,â€¦,1.0}`).
3) For each `r`, define the cut-set `S_norm(r) = {z : Ï€_norm(z) â‰¥ r}`.

For continuous `z`, if `S_norm(r)` is an interval `[â„“(r), u(r)]`, define a generic â€œset scoreâ€

`SS(r) = (u(r)-â„“(r)) + c(r)Â·d(y, S_norm(r))`

where `d(y,S)` is distance from `y` to the set (0 if inside).

Then define the overall score

`Score = ÎºÂ·I + Î£_{râˆˆğ“¡} w(r)Â·SS(r)`.

Notes:

- This is not claiming probabilistic â€œpropernessâ€; itâ€™s a reasonable engineering score.
- It behaves sensibly under subnormality because sharpness uses `Ï€_norm` shape, while `I` is priced separately.
- If you want to emphasize â€œcoreâ€ plausibility, pick weights `w(r)` increasing in `r` and penalty scale `c(r)` increasing in `r`.

### 3B) Option 2: Coverage-calibrated PWIS (best for fair comparison to probability/quantile forecasts)

If you want fairness vs probabilistic forecasts, you really want cut-sets that correspond to **comparable empirical coverages**.

Recipe:

1) Work with normalized `Ï€_norm` and define cut-sets `S_norm(r)` for `r âˆˆ (0,1]`.
2) On a calibration dataset, estimate the **coverage function**

`cov(r) = mean_t 1{ y_t âˆˆ S_norm,t(r) }`.

Because cut-sets shrink as `r` increases, `cov(r)` should decrease as `r` increases (roughly).

3) Choose target coverages `p âˆˆ {0.5, 0.8, 0.9}` (or whatever youâ€™ll also use for probabilistic models).
4) For each `p`, find the cut level `r(p)` such that `cov(r(p)) â‰ˆ p`.
5) Use the standard **interval score** at nominal miscoverage `Î±_prob = 1 - p` on the interval `S_norm(r(p))`.

Finally:

`WIS_like = ÎºÂ·I + Î£_{p} v(p)Â·IS_{Î±_prob}( S_norm(r(p)), y )`.

This produces a score that is *structurally identical* to WIS for probabilistic central prediction intervals, so comparisons are much cleaner.

What you gain:

- interpretability (â€œthis set is aiming for 80% coverageâ€)
- comparability to quantiles/ensembles
- a clear calibration diagnostic (the `cov(r)` curve)

### 3C) Option 3: Threshold event score using (necessity, possibility) as an interval forecast

For an event `A_T = {y â‰¥ T}`, and normalized `Ï€_norm`:

- `u = Î _norm(A_T)` (â€œcould exceedâ€)
- `l = N_norm(A_T) = 1 - Î _norm(A_T^c)` (â€œmust exceedâ€)

This gives an interval `[l,u]` that you can read as:

> â€œthe true exceedance probability is somewhere between `l` and `u`â€

Then an undergrad-friendly â€œinterval log scoreâ€ is:

`S_T =  eÂ·[-log(u+Îµ)] + (1-e)Â·[-log(1-l+Îµ)] + Î»Â·(u-l) + ÎºÂ·I`

where `e = 1{y â‰¥ T}`.

This matches the common imprecise-probability idea: if the event happens you get penalized for having too small an upper bound; if it doesnâ€™t happen you get penalized for having too large a lower bound; width `(u-l)` penalizes vagueness.

### 3D) Option 4: Ordered-category possibilistic score (RPS-like, no continuous curve needed)

If Clyfar outputs category possibilities for ordered categories `1..K`, you can build *cumulative* upper/lower bounds (for â€œy â‰¤ kâ€ events):

`U_k = Î _norm(y â‰¤ k) = max_{iâ‰¤k} Ï€_norm(i)`

`L_k = N_norm(y â‰¤ k) = 1 - max_{i>k} Ï€_norm(i)`

Observation indicator:

`O_k = 1{ y_obs â‰¤ k }`.

Now score how far the observation is from the interval `[L_k, U_k]` across all cutpoints:

`S = Î£_{k=1}^{K-1} dist(O_k, [L_k, U_k])^2 + Î»Â·Î£_{k=1}^{K-1} (U_k - L_k) + ÎºÂ·I`

where `dist(x,[a,b]) = 0` if `xâˆˆ[a,b]`, else distance to the nearer endpoint.

Why this is appealing:

- Itâ€™s the same spirit as the ranked probability score (RPS) but for bounds.
- It works directly on discrete categories (no need for continuous ozone grid).

### 3E) Option 5: Tail-risk ranking score (captures the â€œtop few % extreme possibilityâ€ idea)

If the scientific value is partly â€œcan the model flag rare-but-important days even when centroids are wrong?â€, then **ranking** metrics are natural.

Example for a threshold `T` (like 70 ppb):

1) Define observed event `e_t = 1{y_t â‰¥ T}`.
2) Define a ranking score from Clyfar, such as `s_t = Î _norm,t(A_T)` (or `s_t = Ï€_norm,t(extreme)`).
3) Evaluate how well `s_t` ranks the exceedance days:
   - ROC-AUC (general ranking skill), and/or
   - Precisionâ€“Recall AUC (better when exceedances are rare).

This avoids pretending `s_t` is a probability; it just asks: â€œdo bigger possibility values correspond to more frequent events?â€

Optional: assess whether `I_t` modifies usefulness:

- Compare AUC on all cases vs AUC restricted to â€œlow-ignoranceâ€ cases (`I_t â‰¤ Ï„`).
- Or build a composite rank score like `s_t = (1-I_t)Â·Î _norm,t(A_T)` if you want â€œhigh risk *and* model confidentâ€.

### 3E) Optional stretch: convert possibility â†’ probability for â€œclassicâ€ scores (use with caution)

If you truly need a single comparable score like CRPS/log-score across everything, you can convert the possibilistic forecast to a probability distribution `p(z)` and then score `p` in the usual way.

But you must be honest: **this conversion injects assumptions** (it collapses second-order uncertainty).

Some possible conversions (in increasing â€œassumption strengthâ€):

1) **Normalize-by-sum**: `p_i = Ï€_i / Î£_j Ï€_j` (simple, but not theoretically justified in general).
2) **Softmax**: `p_i âˆ exp(Ï„Â·Ï€_i)` (tune Ï„; still an assumption).
3) **Max-entropy under constraints**: choose `p` that maximizes Shannon entropy subject to compatibility constraints implied by `Ï€` (more principled, but heavier).

If you go this route, Iâ€™d strongly recommend reporting both:

- the classic probabilistic score on `p`, and
- an â€œignorance / abstentionâ€ statistic like `I`.

## 4) â€œInformation theoryâ€ in possibility space (whatâ€™s the analog of Shannon entropy?)

There isnâ€™t one single universally agreed â€œpossibility entropyâ€ the way Shannon entropy is standard for probability.

But there *are* well-motivated uncertainty measures, usually split into:

1) **Nonspecificity** (how large is the plausible set?)  
2) **Fuzziness/ambiguity** (how graded is the membership/possibility?)

### 4.1 Undergrad-friendly nonspecificity (layer-cake / area view)

For normalized `Ï€_norm` on a fixed domain of length `L`:

`NS_shape = (1/L) âˆ« Ï€_norm(z) dz`

Interpretation: average plausibility across the domain; smaller means more of the domain is being ruled out.

Equivalently (same number, different intuition):

`âˆ« Ï€_norm(z) dz = âˆ«_0^1 |S_norm(r)| dr`

Interpretation: average size of the Î±-cut sets across strictness levels.

### 4.2 A simple â€œsurprisalâ€ idea that still makes sense

Even though `Ï€(y)` is not a probability, the quantity

`surprise(y) = -log(Ï€_norm(y) + Îµ)`

still behaves like â€œhow incompatible was the observation with what the model thought was plausibleâ€.

This can be a useful diagnostic term inside a larger score, as long as you donâ€™t claim it is Shannon information.

## 5) Ten questions to clarify what you want (to guide the correction)

These are the questions that most affect what the â€œrightâ€ metric should be.

1) What exact forecast object are we scoring: per-member possibilistic outputs, an ensemble-aggregated possibility curve, or a single â€œscenario meanâ€ possibility?
2) Do you want verification on the full continuous ozone curve `Ï€(z)` (20â€“140 ppb grid in `summary-fuzzy-inference.md`), or only on the 4-category outputs (background/moderate/elevated/extreme)?
3) For the communication goal: is high ignorance `I` supposed to make users **more cautious** (risk-averse) or simply make them **less trusting** of the forecast?
4) Should `I` be penalized (operational usefulness) or can it be â€œgoodâ€ if it happens exactly on hard/tipping-point days (epistemic honesty)?
5) Are your main verification questions about thresholds (e.g., `yâ‰¥70`), about category correctness, or about full-distribution shape?
6) If thresholds matter: which thresholds, and do you want one score per threshold or a pooled multi-threshold score?
7) Do you mostly want a *ranking* claim (â€œextreme possibility is top 2% on episode daysâ€) or a *calibration/coverage* claim (â€œmy 80%-like cut-set contains the truth ~80% of the timeâ€)?
8) If you want fair comparison to probabilistic systems: what will competitors provide (ensembles, full CDFs, or a fixed set of quantiles)?
9) Are you willing to learn a mapping from possibility cut levels `r` â†’ empirical coverage (enables a clean WIS-like comparison), and if so should that mapping be global, seasonal, or lead-time dependent?
10) Do you want a single scalar metric, or a small scorecard (e.g., tail-risk ranking + sharpness + ignorance/self-awareness) that matches the â€œsecond-order uncertaintyâ€ story?
